
In this chapter, we evaluate our quantification model and inference algorithms on simulated datasets with constant expected degradation rates and real direct RNA-seq datasets with sequencing spike-ins. 

\section{Read alignment}

Simulated and real reads were aligned with minimap2 \cite{Minimap2018, Minimap2021}, a popular aligner for long reads. For alignment to the genome, we used the flags \texttt{-ax splice -uf -k14} that take into account splicing and forward strand alignment for ONT direct RNA-seq as recommended by minimap2 developers. For alignment to the transcriptome, we use default flags \texttt{-ax map-ont -N10} for mapping ONT reads, keeping 10 secondary alignments due to similarities in transcript isoform sequences. We used the same genomic or transcriptomic alignments across the tools, depending on which is required as input.   

\section{Model variations}

We compare two variations of our model based on the read length-isoform agreement:

\paragraph{Deg. EM (exact)} We ran our model with the exact read length-isoform agreement model (Eqn. \ref{eq:read-iso-agreement}) and provided the known degradation rate to the model for inference. This is of course unrealistic, since in real data, the degradation rate is not known \textit{a prior} and there is no bound on the maximum read length (i.e., the degradation rate is not constant). Nevertheless, this serves as a useful sanity check that our approach works. 

\paragraph{Deg. EM (emp.)} The second variation of our model uses the empirical read length-isoform agreement model (Eqn. ) and estimates the degradation rate per base from the data. This variation has no limitation on the maximum read length nor constraints on the degradation rate. 

\section{Methods for benchmarking}

We benchmark our model against three existing methods for transcript quantification from long-read RNA-seq. For brevity, detailed descriptions of these methods are omitted here but included in Appendix \ref{ap:meth}.

\paragraph{Bambu} Bambu (manuscript in review) \cite{Bambu2022} is a method for reference guided transcript discovery and quantification for long read RNA-seq data. Crucially, Bambu is one of the few existing long-read quantification methods that models degradation bias. For benchmarking, we ran Bambu (v.2.0.6) without transcript discovery. For transcript quantification, we ran Bambu with bias correction (default) and without bias correction (\texttt{degradationBias=FALSE}). We used the defaults for all other parameters.

\paragraph{FLAIR} Full-Length Alternative Isoform analysis of RNA (FLAIR) \cite{Tang2020} is a method for correction, isoform definition and alternative splicing analysis of long reads. For benchmarking we ran FLAIR (v.1.5.1) with the modules \texttt{correct}, \texttt{collapse} and \texttt{quantify}. When running FLAIR on simulated data, we set \texttt{--support 1} for FLAIR \texttt{collapse}, keeping isoforms that are supported by minimally one read (default=3).  

\paragraph{NanoCount} NanoCount \cite{Gleeson2021} is a method for quantifying isoform abundance from ONT direct RNA-seq data. Of the three methods listed here, it is the most comparable to our model as it is tailored for direct RNA-seq and uses an expectation maximization algorithm for estimating isoform abundance estimates. For benchmarking, we ran NanoCount (v.1.0.0.post6) with default parameters.

\section{Evaluations on simulated data}\label{sec:eval-sim}

To evaluate our model's ability to correct for degradation bias, we simulated five datasets for a range of degradation rates $\mathbb{E}[d]\in\{0.05,0.1,0.2,0.4,0.5\}$ (Appendix \ref{ap:sim-deg-reads}). In addition, we simulated reads for artificial novel isoforms that are modified by dropping exons from the 5' end of selected reference isoforms, termed \textit{subset} isoforms (Appendix \ref{ap:gen-novl-iso}, Fig. \ref{fig:app-a-1}). This increases the proportion of multi-mapping reads and makes correcting for degradation bias crucial for accurate transcript quantification. 

The read counts for simulation follow a negative binomial distribution, which is often used for modeling RNA-seq counts and other count data that is over-dispersed, i.e., where the assumption of equal mean and variance is not held \cite{Cameron2013, Anders2010, Robinson2010}. We parameterise the distribution with the mean $\mu$ and a dispersion parameter $\alpha$ such that the variance is given by $\mu+\alpha\mu^2$. This is the same parameterisation used in \cite{Robinson2010}. To ensure that the negative binomial is a valid choice of distribution, we fit discrete distributions to the counts returned by existing methods on real long-read RNA-seq data, and find that the negative binomial provides a better fit to the data compared to the Poisson distribution (Appendix \ref{ap:count-dist}).   

\subsection{Comparisons between model variations}

In this section, we compare the Deg. EM (exact) and Deg. EM (emp.) models based on the isoform abundance estimates obtained. We compare these estimates against the simulated ground truth based on Spearman correlation (SCC), normalized root-mean-squared error (NRMSE) and median relative difference (MRD). Explanations of these metrics can be found in Appendix \ref{ap:eval-metrics}.  

Both variations of the model perform comparably well on the simulated data, achieving SCC $>$ 0.7 across all five simulated datasets with low NRMSE and MRD. Table \ref{tab:summary-1} shows the mean of each metric across the five datasets for all and subset isoforms. We first note that performance on the subset isoforms is poorer compared to that on all isoforms, for both variations and across metrics. This is expected as there is more ambiguity in read assignment with the subset isoforms. 

% Table generated by Excel2LaTeX from sheet 'sec-4-1-table'
\begin{table}[htbp]
\centering
  \resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|P{2cm}|P{2cm}|P{2cm}|P{2cm}|P{2cm}|P{2cm}|}
\cline{2-7}    \multicolumn{1}{c|}{} & \multicolumn{3}{c|}{All isoforms} & \multicolumn{3}{c|}{Subset isoforms} \bigstrut\\
\hline
Method & SCC   & NRMSE & MRD   & SCC   & NRMSE & MRD \bigstrut\\
\hline
Deg. EM (exact) & 0.832 & 0.455 & \textbf{0.006} & \textbf{0.793} & 0.711 & \textbf{0.162} \bigstrut\\
\hline
Deg. EM (emp.) & \textbf{0.852} & \textbf{0.416} & 0.016 & 0.783 & \textbf{0.446} & 0.277 \bigstrut\\
\hline
\end{tabular}%
}
\caption[Summary of metrics across simulated datasets for model variations]{Summary of metrics across simulated datasets for model variations. We report the mean SCC, NRMSE and MRD across the five datasets for all isoforms and subset isoforms separately. Bold values indicate the best performance for each column.}
\label{tab:summary-1}
\end{table}%

Next, we examine the performance on each dataset separately by examining performance on each metric across the datasets for all and subset isoforms (Fig. \ref{fig:4-1-scc-nrmse}). We observe an interesting trend in both the SCC and NRMSE: as the degradation rate increases, the empirical model performs slightly better compared to the exact model. In contrast, the exact model dominates the empirical model in terms of the MRD, especially for subset isoforms.  

To investigate this further, we visualised estimates and fitted a kernel density estimate on the subset isoforms only for datasets with degradation rates 0.2 and 0.5 (Fig. \ref{fig:4-1-scatter}). On a dataset with relatively lower degradation rate (0.2), the exact model performs qualitatively well, with the density of points over the diagonal (Fig. \ref{fig:4-1-scatter}a), while the empirical model underestimates counts for certain isoforms (Fig. \ref{fig:4-1-scatter}b). However, for higher degradation rates (0.5), the exact model appears to aggressively overestimate counts for a subset of isoforms (Fig. \ref{fig:4-1-scatter}c) compared to the empirical model, where the counts are moderately distributed about the diagonal (Fig. \ref{fig:4-1-scatter}d). This explains the significant increase in NRMSE in the exact model for larger degradation rates compared to the empirical model (Fig. \ref{fig:4-1-scc-nrmse}) because the NRMSE penalizes large errors more strongly than moderate errors. 

Based on our analyses in section \ref{sec:deg-est}, we observed that degradation rates of 0.1-0.2 from direct RNA-seq data. In this range, the two variations perform comparably, and have a high SCC with each other ($\mathbb{E}[d]$=0.1, SCC=0.957, $\mathbb{E}[d]$=0.2, SCC=0.909).  

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/sec-4-1-scc-nrmse.png}
    \caption[SCC, NRMSE and MRD across simulated datasets for model variations]{SCC, NRMSE and MRD across simulated datasets for model variations. Here, each point is a dataset with constant expected degradation $\mathbb{E}[d]=\{0.05,0.1,0.2,0.4,0.5\}$. \textbf{a.} SCC on all isoforms. \textbf{b.} SCC on subset isoforms. \textbf{c.} NRMSE on all isoforms. \textbf{d.} NRMSE on subset isoforms. \textbf{f.} MRD on all isoforms. \textbf{e.} MRD on subset isoforms.}
    \label{fig:4-1-scc-nrmse}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/sec-4-1-scatter-hard.png}
    \caption[Scatter plots across simulated datasets for model variations]{Scatter plots of the simulated and estimated counts log2 scale across simulated datasets with $\mathbb{E}[d]=0.2$ and $\mathbb{E}[d]=0.5$ for the exact and empirical model. \textbf{a.} Exact model with degradation 0.2. \textbf{b.} Empirical model with degradation 0.2. \textbf{c.} Exact model with degradation 0.5. \textbf{d.} Empirical model with degradation 0.5.}
    \label{fig:4-1-scatter}
\end{figure}

\subsection{Comparisons with existing methods}

We now compare our model against Bambu with and without bias modeling, FLAIR and NanoCount on the simulated datasets, comparing the estimates returned by each method with the simulated ground truth. We analyse counts for all isoforms that we included in the simulation and where the sum of counts across the methods is greater than zero. All methods perform reasonably well on the simulated data across all isoforms, achieving SCC$>$0.69 across all five simulated datasets (Table \ref{tab:summary-2}). NRMSE across the methods are comparable, but MRD for the other methods are about one order of magnitude larger than those attained by our model. However, we observe stark differences between the different methods on the subset isoforms. In particular, methods that do not model and correct for bias (Bambu (no bias), FLAIR, NanoCount) perform considerably poorer than methods that do.   

\begin{table}[htbp]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|P{2cm}|P{2cm}|P{2cm}|P{2cm}|P{2cm}|P{2cm}|}
\cline{2-7}    \multicolumn{1}{r|}{} & \multicolumn{3}{c|}{All isoforms} & \multicolumn{3}{c|}{Subset isoforms} \bigstrut\\
\hline
Method & SCC   & NRMSE & MRD   & SCC   & NRMSE & MRD \bigstrut\\
\hline
Deg. EM (exact) & 0.845 & 0.449 & \textbf{0.002} & \textbf{0.815} & 0.699 & \textbf{0.135} \bigstrut\\
\hline
Deg. EM (emp.) & \textbf{0.863} & \textbf{0.41} & 0.012 & 0.801 & \textbf{0.439} & 0.241 \bigstrut\\
\hline
Bambu & 0.771 & 0.599 & 0.101 & 0.671 & 1.009 & 0.36 \bigstrut\\
\hline
Bambu (no bias) & 0.739 & 0.637 & 0.107 & 0.432 & 1.165 & 0.926 \bigstrut\\
\hline
FLAIR & 0.696 & 0.697 & 0.081 & 0.153 & 1.176 & 0.974 \bigstrut\\
\hline
NanoCount & 0.759 & 0.566 & 0.105 & 0.408 & 1.056 & 0.931 \bigstrut\\
\hline
\end{tabular}%
}
\caption[Summary of metrics across simulated datasets for different methods]{Summary of metrics across simulated datasets for different methods. We report the mean SCC, NRMSE and MRD across the five datasets for all isoforms and subset isoforms separately. Bold values indicate the best performance for each column.}
\label{tab:summary-2}
\end{table}%

Next, we examine the performance on each dataset separately for all the methods on all and subset isoforms separately (Fig. \ref{fig:4-2-scc-nrmse}). Both Deg. EM (exact) and Deg. EM (empirical) improve the performance on SCC compared to all methods on the simulated data (Fig \ref{fig:4-2-scc-nrmse}a,b). Notably, Bambu with bias modeling improves the performance over the whole range of degradation rates compared to Bambu without bias modeling. We note also that Bambu's performance drops quickly as the degradation rate increases, as its degradation rate is calibrated to around 0.1-0.2.   

\newpage

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/sec-4-2-scc-nrmse.png}
    \caption[SCC, NRMSE and MRD across simulated datasets for different methods]{SCC, NRMSE and MRD across simulated datasets for different methods. Here, each point is a dataset with constant expected degradation $\mathbb{E}[d]=\{0.05,0.1,0.2,0.4,0.5\}$. \textbf{a.} SCC on all isoforms. \textbf{b.} SCC on subset isoforms. \textbf{c.} NRMSE on all isoforms. \textbf{d.} NRMSE on subset isoforms. \textbf{f.} MRD on all isoforms. \textbf{e.} MRD on subset isoforms.}
    \label{fig:4-2-scc-nrmse}
\end{figure}

\newpage

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/sec-4-2-scatter-hard.png}
    \caption[Scatter plots across simulated datasets for different methods]{Scatter plots of the simulated and estimated counts log2 scale across simulated datasets with $\mathbb{E}[d]=0.2$ for all methods. \textbf{a.} Deg. EM (exact) model. \textbf{b.} Deg. EM (emp.) model. \textbf{c.} Bambu with bias modeling. \textbf{d.} Bambu with no bias modeling. \textbf{e.} FLAIR. \textbf{f.} NanoCount.}
    \label{fig:4-2-scatter}
\end{figure}

\newpage

\subsection{Runtime analysis}

We measured the end-to-end runtime taken for all methods (Fig. \ref{fig:runtime}) excluding the read alignment step. Bambu and FLAIR were run with 12 threads. NanoCount and the current verison of our model do not implement parallelisation. Across the five datasets, NanoCount was the fastest, followed by Deg. EM (exact). Bambu and FLAIR performed similarly, though FLAIR had a much larger variance in the runtime. Even though the time complexity for both Deg. EM (exact) and Deg. EM (emp.) is $\mathcal{O}(N_A)$, where $N_A$ is the number of alignments, Deg. EM (emp.) is much slower than all other methods because calculation of the empirical read length-isoform agreement probabilities is computationally intensive. Even though the runtimes for the empirical model are longer, they are still tolerable in absolute terms, and comes with improvements in accuracy for accurately quantifying degraded reads. Nevertheless, this is one area for improvement for future iterations of our model.  

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/sec-4-runtime.png}
    \caption[Runtime across simulated datasets for different methods]{Runtime (mins) across simulated datasets for different methods. Times are log10 transformed.}
    \label{fig:runtime}
\end{figure}

\section{Evaluations on real data}

Six H9 samples (600ng mRNA + 1\% spike-in of 6ng SIRV-4) sequenced with direct RNA-seq.

\subsection{Comparisons with existing methods}