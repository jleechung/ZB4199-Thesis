
In this chapter, we specify a generative model for transcript quantification from long-read direct \gls{rnaseq} that accounts for bias due to degradation of \gls{mrna} transcripts. We detail the assumptions of our model, formulate the generative process and derive statistical algorithms for inference of the parameters of the model.  

\section{Model assumptions}\label{sec:model-assumptions}

The input to our model consists of reads obtained from sequencing transcripts with the \gls{ont} direct \gls{rnaseq} protocol aligned to the reference transcriptome. We assume there is a bijective mapping between reads and transcript, i.e., each read originates from only one transcript, and each transcript generates only one read. Reads are generated independently and identically distributed (iid) from a distribution to be specified, and the 3' end of each read aligns within some region close to the 3' end of the isoform it originated from.

We make two assumptions on the degradation rate $d$. First, we assume that the degradation of a transcript does not depend on the expression level or biotype of the isoform it originated from. Second, we assume that the expected degradation rate is constant, i.e, $\mathbb{E}[d]=c\in(0,1)$. Even though the second assumption is relatively strong (see Section \ref{sec:read-length-isoform-agreement}), we make this simplifying assumption to show that our model works on simulated data, and relax this assumption to allow our model to generalize to real datasets. 

\section{Generative model}\label{sec:generative-model}

Our model aims to capture the generative process of the observed degraded reads from their isoforms of origin. Here, we formalize our model and derive the complete data log likelihood for maximum likelihood estimation of the model parameters. 

\subsection{Notation and formulation}\label{sec:notation-and-formulation}

Let $N$ be the number of observed reads and $M$ be the number of isoforms the reads were generated from. 
\begin{itemize}
    \item We observe a set of reads $\bm{R}=\{r_1,...,r_N\}$, with $r_i$ representing the $i$\ts{th} read. The variable $r_i$ can be thought of as a collection of properties about the $i$\ts{th} read that are relevant for quantification, such as length, start and end positions or GC content.
    \item Our goal is to infer the unknown relative abundances of the isoforms $\bm{\theta}=\{\theta_1,...\theta_M\}$. These relative abundances sum to one and are non-negative, i.e., $\Sigma_j\theta_j=1, \theta_j\geq0$ $\forall j$. 
    \item We introduce $N$ latent variables $\bm{Z}=\{z_1,...,z_N\}$, where $z_i=(z_{i1}$ $...$ $z_{iM})^T$ is a binary vector in $M$-dimensions with $\Sigma_jz_{ij}=1$. $z_i$ describes the assignment of $r_i$ to one of $M$ isoforms, where
    \begin{equation}
        z_{ij} = 
        \begin{cases}
        1 & \text{if } r_i \text{ originated from isoform } j\\
        0 & \text{otherwise }
        \end{cases}
    \end{equation}
\end{itemize} 
The directed graphical model in Figure \ref{fig:graphical-model-1} illustrates the relationship between the variables $\bm{R}, \bm{Z}$ and parameters $\bm{\theta}$, and provides an easy way to read off the factorization of the joint distribution over the observed and latent variables:
\begin{equation}
    p(\bm{R},\bm{Z})=p(\bm{R}\mid\bm{Z})\cdot p(\bm{Z})\label{eq:joint-dist}
\end{equation}
\begin{figure}[H]
    \centering
    \begin{tikzpicture}[scale=1.25, every node/.style={transform shape}]
        \node[const](theta){$\bm{\theta}$}; % 
        \node[latent, right=of theta](z){$z_i$}; %
        \node[obs, right=of z](r){$r_i$}; %
        \edge{theta}{z}; %
        \edge{z}{r}; %
        \plate[inner sep=0.4cm, xshift = -0.1cm]{plate1}{(z)(r)}{$i=1,...,N$}; %
    \end{tikzpicture}
    \caption[Graphical model for long-read RNA-seq]{Graphical model for long-read RNA-seq. Nodes are random variables, and directed edges represent the dependencies between them. Shaded nodes are observed, while unshaded nodes are latent. Variables within the plate are replicated $N$ times.}
    \label{fig:graphical-model-1}
\end{figure}
\noindent The distribution over the latent variables is categorical:
\begin{equation}
    p(z_i)=\prod_{j=1}^M \theta_j^{z_{ij}}\label{eq:latent-dist}
\end{equation}
The conditional distribution of observing the read $r_i$ given $z_i$ is
\begin{equation}
    p(r_i\mid z_i)=\prod_{j=1}^M p(r_i\mid z_{ij})^{z_{ij}}\label{eq:cond-dist}
\end{equation}
We now model the probability that read $i$ originated from isoform $j$, i.e., $p(r_i\mid z_{ij}=1)$. This probability is proportional to:
\begin{itemize}
    \item The alignment compatibility $a_{ij}\in(0,1)$ between read $i$ and isoform $j$, which measures the alignment score (AS) of read $i$ against isoform $j$ scaled by the best alignment score of read $i$ against all isoforms:
    \begin{equation}
        a_{ij} = \frac{{\textrm{AS}}_{ij}}{\max_{j'} {\textrm{AS}}_{ij'}}
    \end{equation}
    \item The probability of observing a read of length $\ell_i$ given the degradation rate $d_j$ of isoform $j$, i.e., $p(\ell_i\mid d_j, \mathrm{len}(j), z_{ij}=1)$, where $\ell_i$ is the length of $r_i$. We refer to this probability as the \textit{read length-isoform agreement} model.      
\end{itemize}
Combining alignment compatibility and the read length-isoform agreement model, we have 
\begin{equation}
    p(r_i\mid z_{ij}=1) = a_{ij}\cdot p(\ell_i\mid d_j, z_{ij}=1)\label{eq:read-iso-agreement}
\end{equation}
We make some remarks on equation \ref{eq:read-iso-agreement}. First, $a_{ij}$ measures how well read $i$ aligns to isoform $j$ compared to its best alignment. Note that if we do not observe an alignment between read $i$ and isoform $j$, then $\mathrm{AS}_{ij}=0$, $a_{ij}=0$, and thus $p(r_i\mid z_{ij}=1)=0$ as expected. Second, the read length-isoform agreement model captures the expected read length distribution for isoform $j$ given its degradation rate and length. In the absence of degradation, we expect that the lengths of all reads originating from isoform $j$ will be equal to the length len$(j)$ of isoform $j$, i.e., $p(\ell_i=\mathrm{len}(j)\mid d_j, z_{ij}=1)=1$ and $0$ otherwise.   

\subsection{Read length-isoform agreement}\label{sec:read-length-isoform-agreement}

We specify the read length-isoform agreement model with the assumption that the expected degradation is constant, i.e., $\mathbb{E}[d]=c\in(0,1)$. For example, if the degradation rate for all transcripts is $d_j=0.2$, then for every kb from the 3' end, we expect 20\% less reads. A constant expected degradation rate  implies that the maximum possible sequenced read length $\ell_\mathrm{max}$ is bounded, and is given by:
\begin{equation}
    -c\cdot \ell_\mathrm{max} + 1 = 0 \implies \ell_\mathrm{max} = 1/c
\end{equation}
For instance, a $\mathbb{E}[d]=0.2$ implies that $\ell_\mathrm{max}=5$ kb. The assumption made here is unrealistic, given that \gls{ont} devices are capable of sequencing reads upwards of $10$ kb in length. Nevertheless, this simplifying assumption enables us to test a simpler read length-isoform agreement model on simulated datasets. We develop a more flexible model allowing for variable degradation in the following section.  

Let the length of isoform $j$  in kb be len$(j)$. For constant expected degradation, we have
\begin{equation}
    p(\ell_i\mid d_j, z_{ij}=1) = 
    \begin{cases}
        \dfrac{d_j}{\mathrm{len}(j)\cdot10^3} & \textrm{if } \ell_i<\min(\mathrm{len}(j), \ell_\mathrm{max})\\
        1-d_j\cdot\mathrm{len}(j) & \textrm{if } \ell_i=\mathrm{len}(j)
    \end{cases}
\end{equation}

\section{Parameter inference}

In this section, we derive an expectation maximization algorithm to infer the parameters $\bm{\theta}$ that maximize the likelihood of the observed data.

\subsection{Likelihood formulation}\label{sec:likelihood-formulation}

The marginal distribution of $r_i$ is obtained by summing the joint distribution (Eq. \ref{eq:joint-dist}) over all $M$ possible states of $z_i$. From Eqs. \ref{eq:latent-dist}, \ref{eq:cond-dist} and \ref{eq:read-iso-agreement}, we obtain
\begin{equation}
    p(r_i)=\sum_{z_i}p(z_i)\cdot p(r_i\mid z_i)=\sum_{j=1}^M \theta_j\cdot a_{ij}\cdot p(\ell_i\mid d_j, z_{ij}=1)
\end{equation}
Since we assume the reads are generated iid, the likelihood is a product over $N$ reads:
\begin{equation}
    p(\bm{R}\mid\bm{\theta}) = \prod_{i=1}^N \sum_{j=1}^M \theta_j\cdot a_{ij}\cdot p(\ell_i\mid d_j, z_{ij}=1)\label{eq:lik}
\end{equation}
The corresponding log-likelihood is 
\begin{equation}
    \log p(\bm{R}\mid\bm{\theta}) = \sum_{i=1}^N \log \left[\sum_{j=1}^M \theta_j\cdot a_{ij}\cdot p(\ell_i\mid d_j, z_{ij}=1)\right]\label{eq:log-lik}
\end{equation}
It is difficult to maximize Eq. \ref{eq:log-lik} with respect to $\bm{\theta}$ because the log acts on the sum. If we had access to the latent variables $\bm{Z}$, the likelihood will decompose. To demonstrate this, we write the complete data likelihood as
\begin{equation}
    p(\bm{R},\bm{Z}\mid\bm{\theta})=\prod_{i=1}^N\prod_{j=1}^M \theta_j^{z_{ij}}\cdot\left[a_{ij}\cdot p(\ell_i\mid d_j, z_{ij}=1)\right]^{z_{ij}}\label{eq:comp-lik}
\end{equation}
The corresponding complete data log-likelihood is 
\begin{equation}
    \log p(\bm{R},\bm{Z}\mid\bm{\theta})=\sum_{i=1}^N\sum_{j=1}^M z_{ij}\cdot\log \left[\theta_j\cdot a_{ij}\cdot p(\ell_i\mid d_j, z_{ij}=1)\right]\label{eq:comp-log-lik}
\end{equation}

\subsection{Expectation maximization}\label{sec:em}

The EM algorithm  is a powerful method for finding maximum likelihood estimates of parameters in latent variable models, and is used in many RNA-seq quantification methods for parameter inference \cite{Li2011, Kallisto, Salmon, LIQA, Gleeson2021, Bambu2022}. It consists of a two-stage iterative optimization procedure for maximizing the likelihood. In our setting, we seek to maximize the complete data log-likelihood with respect to $\bm{\theta}$. However, we observe only the incomplete data $\bm{R}$. Since we cannot obtain the complete data log-likelihood (Eq. \ref{eq:comp-log-lik}), we take its expectation under the posterior of the latent variables $\bm{Z}$ given some setting of the parameters $\bm{\theta}^{(t)}$: this corresponds to the E step of the EM algorithm. In the M step, we maximize this expectation with respect to the parameters $\bm{\theta}$ to obtain a new estimate of the parameters $\bm{\theta}^{(t+1)}$.   

We obtain the posterior of the latent variables $\bm{Z}$ from Eqs. \ref{eq:latent-dist} and \ref{eq:joint-dist}:
\begin{equation}
    p(\bm{Z}\mid\bm{R},\bm{\theta})\propto\prod_{i=1}^N\prod_{j=1}^M\left[\theta_j\cdot a_{ij}\cdot p(\ell_i\mid d_j,z_{ij})\right]^{z_{ij}=1}\label{eq:post}
\end{equation}
\paragraph{E-step} We take the expectation of the complete data log-likelihood (Eq. \ref{eq:comp-log-lik}) with respect to the posterior distribution of the latent variables (Eq. \ref{eq:post}):
\begin{equation}
    \mathbb{E}_{\bm{Z}\mid\bm{R},\bm{\theta}}\left[\log p(\bm{R},\bm{Z}\mid\bm{\theta})\right]=\sum_{i=1}^N\sum_{j=1}^M \mathbb{E}[z_{ij}]\cdot\log \left[\theta_j\cdot a_{ij}\cdot p(\ell_i\mid d_j, z_{ij}=1)\right]\label{eq:e-step}
\end{equation}
where 
\begin{equation}
    \mathbb{E}[z_{ij}]=\frac{\theta_j\cdot a_{ij}\cdot p(\ell_i\mid d_j, z_{ij}=1)}{\sum_{j'}\theta_j'\cdot a_{ij'}\cdot p(\ell_i\mid d_{j'}, z_{ij'}=1)}=\gamma_{ij}
\end{equation}

\paragraph{M-step} We maximize the expression in Eq. \ref{eq:e-step} with respect to $\bm\theta$ constrained by $\sum_j\theta_j=1$, which is equivalent to the following:
\begin{equation}
    \mathrm{argmax}_{\bm\theta}\sum_i\sum_j\gamma_{ij}\cdot\log\theta_{ij}
\end{equation}
The Lagrangian is
\begin{equation}
    \mathcal{L}(\theta_j,\lambda)=\sum_i\sum_j\gamma_{ij}\cdot\theta_j + \lambda\left[\sum_j\theta_j-1\right]
\end{equation}
Taking the derivative with respect to $\theta_j$ and setting to $0$ we have
\begin{equation}
    \frac{\partial\mathcal{L}(\theta_j,\lambda)}{\partial\theta_j}=\sum_i\frac{\gamma_{ij}}{\theta_j}+\lambda=0\implies\theta_j=-\frac{\sum_i\gamma_{ij}}{\lambda}\label{eq:derivative}
\end{equation}
Obtain $\lambda$ by observing that
\begin{equation}
    1=\sum_j\theta_j=\sum_j-\frac{\sum_i\gamma_{ij}}{\lambda}=\sum_i-\frac{\sum_j\gamma_{ij}}{\lambda}=\sum_i-\frac{1}{\lambda}=-\frac{N}{\lambda}\implies\lambda=-N\label{eq:lambda}
\end{equation}
From Eqs. \ref{eq:derivative} and \ref{eq:lambda} we find that
\begin{equation}
    \theta_j=\frac{\sum_i\gamma_{ij}}{N}\label{eq:thetaj}
\end{equation}
An intuitive interpretation of Eq. \ref{eq:thetaj} is that the relative abundance of isoform $j$ is the fraction of all reads that are soft-assigned to isoform $j$ by the EM algorithm. In summary, the EM algorithm for our model is as follows:
\begin{enumerate}
    \item Initialize $\bm{\theta}^{(0)}$
    \item E step: compute $\gamma_{ij}^{(t)}=\frac{\theta_j^{(t)}\cdot a_{ij}\cdot p(\ell_i\mid d_j, z_{ij}=1)}{\sum_{j'}\theta_{j'}^{(t)}\cdot a_{ij'}\cdot p(\ell_i\mid d_{j'}, z_{ij'}=1)}$
    \item M step: compute $\theta_j^{(t+1)}=\frac{\sum_i\gamma_{ij}^{(t)}}{N}$
    \item Repeat steps 2 and 3 until convergence.
\end{enumerate}
To initialize $\bm\theta$, we assign equal relative abundances to all isoforms by setting $\theta_j=1/M$ $\forall j$. We test for convergence by computing changes in the parameters or the log-likelihood in Eq. \ref{eq:log-lik} after each iteration of the algorithm. Once the change in the  parameters or log-likelihood falls below a certain threshold, we terminate the algorithm. Proof of the concavity of the log-likelihood function, and thus the optimality of the inferred parameters, can be found in the Appendix \ref{sec:proof-log-lik}. 
