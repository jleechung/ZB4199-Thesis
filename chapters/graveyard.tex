Third generation sequencing technologies have enabled the production of long reads ranging from tens to thousands of bases in length \cite{Pollard2018}, and have shown promise in resolving many challenges in genomics and transcriptomics \cite{Bolisetty2015, Byrne2017, DeCoster2019, Liu2019, Mantere2019, Nurk2021}. In particular, long-read technologies enable greater insight into the transcriptome and its complexity, which is crucial in understanding the functioning of cells and their biological processes. These technologies allow accurate detection of full-length transcript isoforms and novel splice junctions while mitigating biases associated with short-read technologies, enabling more accurate quantification of reference and novel isoforms. Nevertheless, biases are still present in long-read technologies, albeit to a lesser extent. 

This project focuses on a particular bias present in long-read \gls{rnaseq} referred to as \textit{degradation bias}. This bias arises due to the fact that from the time a transcript is synthesized to when it is sequenced, it is subject to multiple factors that results in its degradation, primarily from the 5' end of the transcript. Consequently, the observed reads are often truncated and \textit{degraded}, potentially resulting in ambiguity in read assignment to transcript isoforms. This, in turn, leads to erroneous quantification estimates. As degradation bias in long-read \gls{rnaseq} has not been extensively covered in existing literature, we attempt to do so here by characterising the bias and its effects on quantification from long-read \gls{rnaseq}, and developing a framework to model and correct such bias by producing \textit{degradation-aware quantification estimates}. We incorporate this model into bambu, a tool for context-aware transcript discovery and quantification from long-read \gls{rnaseq}. 

To evaluate quantification estimates from \gls{rnaseq}, experimental datasets with known ground truth or simulated datasets are typically used, and estimates are evaluated based on their concordance with the ground truth. Here, we generate simulated long-read \gls{rnaseq} data as experimental datasets with known ground truth are unavailable. In order to test our bias models, we develop an approach to generating novel transcript models that is capable of generating adversarial transcript models where not correcting for degradation bias can lead to significantly erroneous quantification estimates. For evaluating degradation-aware quantification estimates, we employ several reference-based and reference-free metrics that measure the concordance of estimates with the ground truth along with the variance of estimates acrosss technical replicates.   

\section{Review}

Here, we review various concepts and existing literature relevant to our aim of modeling bias in long-read \gls{rnaseq}. We first examine (i) biases in short-read \gls{rnaseq} technologies and how they are accounted for by existing methods, which provide some ideas on how to handle biases in long-read \gls{rnaseq}. Next, we review (ii) long-read \gls{rnaseq} technologies and how they mitigate biases in (i), and (iii) biases that long-read technologies themselves possess. 

\subsection{Bias in short-read \gls{rnaseq}}

Short-read \gls{rnaseq} technologies enable deep sequencing of highly accurate short reads, and has been the dominant technology for profiling the transcriptome since its popularisation in the early 2010s \cite{Lowe2017}. Typically, library preparation protocol for an \gls{rnaseq} experiment following \gls{rna} extraction involves \gls{rna} fragmentation, followed by the use of random hexamer primers for priming of the fragments to synthesize one strand of \gls{cdna}. After second strand synthesis, the resulting double stranded \gls{cdna} are size-selected and amplified via \gls{pcr} amplification to generate enough \gls{cdna} for sequencing \cite{Marguerat2010}.  

Biases in short-read \gls{rnaseq} have been extensively studied \cite{Hansen2010, Li2010, Li2011, Zhengpeng2010, Roberts2011, Benjamini2012, Lahens2014, Love2016}, and can often be traced back to specific steps in the protocol described above. For instance, \cite{Hansen2010} showed that priming with random hexamer primers induces bias in the nucleotide composition of the reads and results in non-uniform representation of reads across the length of the transcript. Next, size selection of fragments results in an over-representation of fragments of a certain length, while \gls{rna} degradation and \gls{mrna} selection can lead to over-representation of fragments that are located towards either the beginning or end of the transcript \cite{Roberts2011, Lahens2014, Love2016}. \gls{pcr} amplification before sequencing also introduces bias by preferential amplification of fragments with certain GC content \cite{Benjamini2012, Love2016}. The combination of these biases affect quantification estimates, and if not corrected for, leads to erroneous estimates \cite{Roberts2011, Love2016}. 

Methods to address these biases can be categorised into two broad classes. The first class of methods involve innovations in library preparation methods to reduce bias from their source of origin \cite{Vandijk2014}. For instance, an amplification-free \gls{rnaseq} protocol was proposed in \cite{Mamanova2010}, while thermostable polymerases that exhibit relatively lower GC bias were identified in \cite{Quail2012}. However, reducing bias through new protocols is not always feasible, and thus we focus on the second class of methods that involve modeling and correcting for the bias \textit{in silico}. While there are many computational methods that fall under this class, here we explicate two approaches that are most relevant to our aim, namely the \textit{alpine}\cite{Love2016} method and the use of non-uniform read distributions. 

alpine was proposed in \cite{Love2016} to model \gls{rnaseq} fragment sequence bias. A Poisson generalised linear model is used to model the counts for each fragment while correcting for certain biases. In particular, alpine models the (i) fragment length distribution, (ii) read start preferences (induced by random hexamer priming), (iii) relative position of each fragment in the transcript, (iv) GC content of each fragment and (v) the presence of stretches of high GC content within each fragment. Importantly, these bias terms are estimated using only single-isoform genes in order to avoid probabilistic assignment of fragments to multiple isoforms of a gene. Bias terms for (i) and (ii) are calculated from the data, while coefficients for (iii)-(v) are estimated with natural cubic splines within the \gls{glm}.

A non-uniform read distribution model was proposed in \cite{Zhengpeng2010} to model non-uniform read distribution of reads over the length of the transcript for improved isoform expression inference.  

\subsection{Long-read technologies}

Long-read sequencing has been made possible by the development of biochemistry / biophysical techniques to capture full-length RNA / cDNA [https://f1000research.com/articles/6-100/v2]. Two of the most widely used platforms for long-read sequencing include \gls{pacbio} and \gls{ont}.    

- Inception / history of long reads
- Library preparation protocols
- Improvements over short-read (mitigated biases)

\subsection{Bias in long-read \gls{rnaseq}}

\lipsum[3]

\section{Organisation}

The chapters of this thesis are organised as follows:
\begin{itemize}
    \item In Chapter 2, we describe methods to evaluate \gls{rnaseq} quantification, including reference-based metrics where the ground truth is known, and reference-free metrics which describe the variance of estimates across technical replicates. 
    \item In Chapter 3, we develop methods to generate novel transcript models for simulation, including a handcrafting approach that is splice-aware, and an experimental approach that involves deep generative modeling.
    \item In Chapter 4, we examine and characterise degradation bias in long-read \gls{rnaseq} across multiple samples and protocols, demonstrating how it can lead to erroneous quantification estimates. We develop a model that corrects for degradation bias, and demonstrate that it improves quantification estimates on data simulated with approaches in Chapter 3 based on evaluation metrics in Chapter 2. 
    \item In Chapter $n$, we summarise the ideas of the thesis and discuss potential directions of future work. 
\end{itemize}

subsection{Variational Bayesian inference}

In addition to the maximum likelihood approach in section \ref{sec:em}, one can adopt a Bayesian approach to infer the posterior distribution of the parameters $\bm\theta$ given the observed data $\bm{R}$. Here, we treat $\bm\theta$ as a random variable and place a Dirichlet prior parameterised by hyperparameters $\bm\alpha_0\in\mathbb{R}^{M}$ over $\bm{\theta}$ (Eq. \ref{eq:prior}) based on conjugacy with the categorical distribution over the latent variables $\bm{Z}$  (Eq. \ref{eq:bayes-cond}). The graphical model in \ref{fig:graphical-model-2} shows the dependency between all parameters, latent and observed variables.  
\begin{equation}\label{eq:prior}
    p(\bm\theta\mid\bm\alpha_0)\propto \prod_{j=1}^M \theta_j^{\alpha_{0j}-1}
\end{equation}
\begin{equation}\label{eq:bayes-cond}
    p(\bm{Z}\mid\bm\theta)=\prod_{j=1}^M\theta_j^{z_{ij}}
\end{equation}
\begin{figure}[H]
    \centering
    \begin{tikzpicture}[scale=1.25, every node/.style={transform shape}]
        \node[const](alpha){$\bm{\alpha_0}$}; %
        \node[latent, right=of alpha](theta){$\bm{\theta}$}; % 
        \node[latent, right=of theta](z){$z_i$}; %
        \node[obs, right=of z](r){$r_i$}; %
        \edge{alpha}{theta}; %
        \edge{theta}{z}; %
        \edge{z}{r}; %
        \plate[inner sep=0.4cm, xshift = -0.1cm]{plate1}{(z)(r)}{$i=1,...,N$}; %
    \end{tikzpicture}
    \caption[Bayesian graphical model for long-read RNA-seq]{Bayesian graphical model for long-read RNA-seq. Under this model, $\bm\theta$ is a random variable distributed according to $\textrm{Dir}(\bm{\alpha_0})$.}
    \label{fig:graphical-model-2}
\end{figure}
\noindent The posterior probability of the parameters and latent variables given the observed data is 
\begin{equation}\label{eq:post-full}
\begin{split}
    p(\bm{\theta},\bm{Z}\mid\bm{R}) & =\frac{p(\bm{R},\bm{Z},\bm{\theta})}{p(\bm{R})} \\
    & = \frac{p(\bm{R}\mid\bm{Z})\cdot p(\bm{Z}\mid\bm{\theta})\cdot p(\bm{\theta})}{p(\bm{R})} \\
    & = \frac{p(\bm{R}\mid\bm{Z})\cdot p(\bm{Z}\mid\bm{\theta})\cdot p(\bm{\theta})}{\int_\theta \sum_{\bm{Z}} p(\bm{R}\mid\bm{Z})\cdot p(\bm{Z}\mid\bm{\theta})\cdot p(\bm{\theta})\cdot d\theta}
\end{split}
\end{equation}
Direct evaluation of the posterior is intractable, since evaluating the denominator of the final expression in Eq. \ref{eq:post-full} requires summation over the space of $\bm{Z}$, which is exponential ($M^N$) in size. We turn to approximate inference methods for posterior inference. In particular, we consider a variational approximation to the posterior $q(\bm{Z},\bm{\theta})$ that is mean-field, such that the variational distribution factorizes:
\begin{equation}
    q(\bm{Z},\bm{\theta})=q(\bm{Z})\cdot q(\bm\theta)
\end{equation}
Here, we present the optimal variational distributions for $\bm{Z}$ and $\bm\theta$ that minimize the KL divergence to the posterior. Full derivations are presented in Appendix \ref{sec:variational-dist} for brevity. The optimal variational distribution for $\bm{Z}$ is
\begin{equation}\label{eq:opt-z-1}
    q(\bm{Z})=\prod_{i=1}^N\prod_{j=1}^M \gamma_{ij}^{z_{ij}}
\end{equation}
Here, $\gamma_{ij}$ is defined by 
\begin{equation}\label{eq:opt-z-2}
    \gamma_{ij}=\frac{\rho_{ij}}{\sum_j\rho_{ij}}
\end{equation}
with
\begin{equation}\label{eq:opt-z-3}
    \log\rho_{ij}=\log\left[a_{ij}\cdot p(\ell_i\mid d_j,z_{ij}=1)\right]+\psi(\alpha_j)-\psi\left(\sum_{j=1}^M\alpha_j\right)
\end{equation}
where $\psi(\cdot)$ is the digamma function. 
The optimal variational distribution for $\bm\theta$ is
\begin{equation}\label{eq:opt-theta-1}
    q(\bm\theta) = \mathrm{Dir}(\bm\alpha)
\end{equation}
where the $j$\ts{th} component of $\bm\alpha$ is given by
\begin{equation}\label{eq:opt-theta-2}
    \alpha_j=\alpha_{0j}+\sum_{i=1}^N\gamma_{ij}
\end{equation}
From Eqs. \ref{eq:opt-z-1} and \ref{eq:opt-theta-1}, we perform coordinate ascent inference by iteratively optimizing each variational distribution holding the other fixed. This results in an iterative optimization procedure analogous to the EM algorithm:
\begin{enumerate}
    \item Initialize hyperparameters $\alpha_j^{(0)}=\alpha_{0j}$
    \item Update $q(\bm{Z})$: compute $\gamma_{ij}^{(t)}$  with $\alpha_j^{(t)}$ from Eqs. \ref{eq:opt-z-2} and \ref{eq:opt-z-3}
    \item Update $q(\bm\theta)$: compute $\alpha_{j}^{(t+1)}$  with $\gamma_{ij}^{(t)}$ from Eq. \ref{eq:opt-theta-2}
    \item Repeat steps 2 and 3 until convergence. 
\end{enumerate}
Upon convergence, the abundance estimate for the $j$\ts{th} isoform can be obtained by taking the expectation of $\theta_j$ under the posterior distribution:
\begin{equation}
    \mathbb{E}[\theta_j] = \frac{\alpha_{0j}+\sum_i\gamma_{ij}}{\sum_j\alpha_{0j}+N}
\end{equation}

One option for setting hyperparameters $\bm{\alpha_0}$ for the prior $p(\bm{\theta}\mid\bm{\alpha_0})$ is by setting all $\alpha_{0j}$ to some constant $\alpha_0$, yielding a symmetric Dirichlet prior. Small values of $\alpha_0$ result in a sparser $\bm\theta$, while larger values result in a more uniform $\bm\theta$. Another option is to place a hyperprior on $\bm{\alpha_0}$. Since $\bm{\alpha_0}>0$, we can consider
\begin{equation}
    \alpha_{0j}\sim\textrm{Gamma}(a,b)
\end{equation}
We evaluate the choice of priors on transcript quantification in the following section. 


\chapter{Derivations for variational distributions}\label{sec:variational-dist}

From \cite{Blei2017}, the optimal mean-field variational distribution for latent variables $\left(Z_{1}, \ldots, Z_{j}\right)$ and observed variable $X$ is given by
\begin{equation}
    \log q_{j}^{*}\left(Z_{j}\right)=\mathbb{E}_{-j}[\log p(X, Z)]\label{eq:blei}
\end{equation}
\noindent Here, the expectation is taken with respect to all variables other than $Z_j$. We apply Eqn. \ref{eq:blei} to our model in Fig. \ref{fig:graphical-model-2}. The optimal variational distribution for latent variable $\boldsymbol{Z}$ is
\begin{equation}
\begin{split}
    \log q(\boldsymbol{Z}) &=\mathbb{E}_{\boldsymbol{\theta}}\left[\log p(\boldsymbol{R}, \boldsymbol{Z}, \boldsymbol{\theta})\right]+\mathrm{ const. } \\
    &=\mathbb{E}_{\boldsymbol{\theta}}[\log p(\boldsymbol{R} \mid \boldsymbol{Z})]+\mathbb{E}_{\boldsymbol{\theta}}[\log p(\boldsymbol{Z} \mid \boldsymbol{\theta})]+\mathrm{ const. } \\
    &=\sum_{i j} z_{i j} \cdot \log \left[a_{i j} \cdot p\left(\ell_{i} \mid d_{j}, z_{i j}=1\right)\right]+\sum_{i}\sum_{ j}\mathbb{E}_{\boldsymbol{\theta}}\left[\log \theta_{j}\right]+\mathrm{ const. } \\
    &=\sum_{i} \sum_{j} z_{i j} \cdot \log \rho_{i j}+\mathrm{ const. }
\end{split}
\end{equation}
where 
\begin{equation}
        \log \rho_{i j} =\log \left[a_{i j} \cdot p\left(\ell_{i} \mid d_{j}, z_{ij}=1\right)\right]+\mathbb{E}_{\boldsymbol{\theta}}\left[\log \theta_{j}\right]
\end{equation}
Since $\boldsymbol{\theta}$ follows a Dirichlet distribution with parameters $\boldsymbol{\alpha}=\left(\alpha_{1}, \ldots, \alpha_{M}\right)$ we have
\begin{equation}
    \mathbb{E}_{\boldsymbol{\theta}}\left[\log \theta_{j}\right]=\varphi\left(\alpha_{j}\right)-\varphi\left(\sum_{j} \alpha_{j}\right)
\end{equation}
where $\varphi(.)$ is the digamma function. Thus, we have
\begin{equation}
    q(\bm{Z})\propto\prod_i\prod_j\rho_{ij}^{z_{ij}}
\end{equation}
Normalizing this distribution, let
\begin{equation}
    \gamma_{i j}=\frac{\rho_{i j}}{\sum_{j} \rho_{i j}}
\end{equation}
Then 
\begin{equation}
    q(\boldsymbol{Z})=\prod_{i} \prod_{j} \gamma_{i j}^{z_{i j}}
\end{equation}

The optimal variational distribution for parameters $\boldsymbol{\theta}$ is

\begin{equation}
\begin{split}
    \log q(\boldsymbol{\theta})&=\mathbb{E}_{\boldsymbol{Z}}[\log p(\boldsymbol{R}, \boldsymbol{Z}, \boldsymbol{\theta})]+\mathrm{const} \\
    &= \mathbb{E}_{Z}[\log p(\boldsymbol{Z} \mid \boldsymbol{\theta})]+\mathbb{E}_{Z}[\log p(\boldsymbol{\theta})]+\mathrm{const} \\
    &= \mathbb{E}_{Z}\left[\sum_{i} \sum_{j} z_{i j} \cdot \log \theta_{j}\right]+\log p(\boldsymbol{\theta})+\mathrm{const} \\
    &=\sum_{i} \sum_{j} \gamma_{i j} \cdot \log \theta_{j}+\sum_{j}\left(\alpha_{0 j}-1\right) \cdot \log \theta_{j}
\end{split}
\end{equation}
We recognise this as a Dirichlet distribution with parameters $\alpha_{0 j}+\sum_{i} \gamma_{i j}$, where $\alpha_{0j}$ is the $j$\ts{th} parameter of the hyperparameter $\bm{\alpha_0}$. 



\begin{table}[H]
  \centering
    \begin{tabular}{|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|}
\cline{2-7}    \multicolumn{1}{l|}{} & Run1  & Run2  & Run3  & Run4  & Run5  & Run6 \bigstrut\\
    \hline
    A549  & 4     & 0     & 0     & 0     & 0     & 0 \bigstrut\\
    \hline
    H9    & 4     & 3     & 0     & 0     & 0     & 0 \bigstrut\\
    \hline
    Hct116 & 3     & 1     & 2     & 1     & 1     & 1 \bigstrut\\
    \hline
    HepG2 & 3     & 1     & 1     & 0     & 0     & 0 \bigstrut\\
    \hline
    MCF7  & 2     & 1     & 0     & 0     & 0     & 0 \bigstrut\\
    \hline
    K562  & 4     & 0     & 0     & 0     & 0     & 0 \bigstrut\\
    \hline
    \end{tabular}%
      \caption[Description of SG-NEx samples across cell lines and sequencing runs]{Description of SG-NEx samples across cell lines and sequencing runs}
  \label{tab:six-six}%
\end{table}%

We make two assumptions on the degradation rate $d$. First, we assume that the degradation of a transcript does not depend on the expression level or biotype of the isoform it originated from. Second, we assume that the expected degradation rate is constant, i.e, $\mathbb{E}[d]=c\in(0,1)$. Even though the second assumption is relatively strong (see Section \ref{sec:read-length-isoform-agreement}), we make this simplifying assumption to show that our model works on simulated data, and relax this assumption to allow our model to generalize to real datasets. 
